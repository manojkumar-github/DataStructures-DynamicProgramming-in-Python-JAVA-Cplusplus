#!/usr/bin.env python
# Copyright (C) Pearson Assessments - 2020. All Rights Reserved.
# Proprietary - Use with Pearson Written Permission Only

"""
1. pandas intensive
2. Data set benchmarking and management
3. NLP and ML and DL research
4. Developing and deploying using ML APIs
5. Data Pipeline extensions


Applied BERT models for feature computations from research to deployment.
 - Bidirectional Encoder Representations from Transformers (BERT) is a
 technique for NLP (Natural Language Processing) pre-training developed by
 Google. BERT was created and published in 2018 by Jacob Devlin and his
 colleagues from Google.[1][2] Google is leveraging BERT to better understand user searches.[3]
 - Used this model for transfer learning techniques to score a response
 between 0 and 1.




"""